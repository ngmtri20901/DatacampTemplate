{"cells":[{"source":"# Evaluate your Model using the F-score","metadata":{},"id":"8ca5c670-8a64-4904-9d0a-4871d7ef3c44","cell_type":"markdown"},{"source":"The F-score or F-measure is a metric for indicating the accuracy of a classification model. After obtaining the precision and recall on your test set, you can calculate the F1-score with following formula:  \n\n$$\nF_{1} = 2 \\cdot \\frac{\\textrm{precision} \\cdot \\textrm{recall}}{\\textrm{precision} + \\textrm{recall}}\n$$  \n\nThe maximum possible value is 1, which indicates a perfect model. If either precision or recall is 0, the $F_{1}$-score is 0 as well.  \nIn the formula above, we state that precision is as important as recall for our application; that is why we write $F_{1}$. We will use this version of the metric in this template via the `scikit-learn`'s function `f1_score()`. However, you can also apply weights to the precision or recall with the [${F_{\\beta}}$-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#:~:text=The%20F%2Dbeta%20score%20is,recall%20in%20the%20combined%20score.).","metadata":{},"id":"8417ac1e-70f9-4cb9-9615-06969f1dccd2","cell_type":"markdown"},{"source":"# Load packages\nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import f1_score\n%config InlineBackend.figure_format = 'retina'","metadata":{},"id":"604e46dc-3f6e-4df9-8705-3d7560f8a4c4","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# Load data from the csv file\ndf = pd.read_csv(\"F1-score_data.csv\")\ndf.head()","metadata":{},"id":"50265658-3ac1-4075-bc11-46a313ddfa40","cell_type":"code","execution_count":2,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PROBABILITY</th>\n","      <th>ACTUAL LABEL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.95</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.03</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.37</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.87</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.35</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PROBABILITY  ACTUAL LABEL\n","0         0.95             1\n","1         0.03             0\n","2         0.37             1\n","3         0.87             1\n","4         0.35             0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"source":"First, we need to convert the probabilities predicted by our model to actual binary predictions. In this example, we will use a threshold of 0.65; predictions under this threshold will be mapped to 0, the other predictions to 1.","metadata":{},"id":"de9f1e9e-3769-4b86-ace1-59b0e153e2e6","cell_type":"markdown"},{"source":"THRESHOLD = 0.65                 # Choose your threshold for which you want to calculate the F-score\n\n# Convert the probability predicted by the model to an actual binary prediction\ndef convert_to_pred(x):\n    if x < THRESHOLD:            # If under threshold,\n        return 0                   # map to 0.\n    else:                        # Else,\n        return 1                   # map to 1.\n\ndf['PREDICTION'] = df['PROBABILITY'].apply(lambda x: convert_to_pred(x))\n\ndf.head()","metadata":{},"id":"aeb163f5-fdbd-42fc-ac6a-08ff6821b626","cell_type":"code","execution_count":3,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PROBABILITY</th>\n","      <th>ACTUAL LABEL</th>\n","      <th>PREDICTION</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.95</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.03</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.37</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.87</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.35</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PROBABILITY  ACTUAL LABEL  PREDICTION\n","0         0.95             1           1\n","1         0.03             0           0\n","2         0.37             1           0\n","3         0.87             1           1\n","4         0.35             0           0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"source":"Now we can go ahead and calculate the $F_{1}$-score using the `scikit-learn` package.","metadata":{},"id":"9e8e69b8-7fd4-4c76-9fb5-797705d745fe","cell_type":"markdown"},{"source":"f1_score(df['ACTUAL LABEL'], df['PREDICTION'])","metadata":{},"id":"1bd83957-09e2-48bb-8b4b-bbfea1b06951","cell_type":"code","execution_count":5,"outputs":[{"data":{"text/plain":["0.7058823529411764"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"source":"The $F_{1}$-score is about 0.7. That is pretty good, but this probably can be increased depending on the threshold that you use. You can change the threshold to try to get a better classification model.","metadata":{},"id":"f595b586-a796-46e6-bba8-85bf0cd31b57","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}