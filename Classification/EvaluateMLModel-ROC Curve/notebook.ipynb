{"cells":[{"source":"# Evaluate your ML Model using a ROC Curve","metadata":{},"id":"8ca5c670-8a64-4904-9d0a-4871d7ef3c44","cell_type":"markdown"},{"source":"After training your machine learning model, you can test its performance over various threshold settings using an ROC (Receiver Operating Characteristics) curve. You can thus also choose the threshold that best fits your application. The curve uses two important metrics: the true positive rate (TPR) and the false positive rate (FPR), along with a variable: the threshold. While this method is very useful, it is limited to classification problems where your model outputs a class probability.","metadata":{},"id":"8417ac1e-70f9-4cb9-9615-06969f1dccd2","cell_type":"markdown"},{"source":"# Load packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\n%config InlineBackend.figure_format = 'retina'","metadata":{},"id":"ad6694a6-f064-44a1-aa77-57095fda33e5","cell_type":"code","execution_count":null,"outputs":[]},{"source":"# Upload your data as CSV and load as data frame\ndf = pd.read_csv(\"ROC_data.csv\")\ndf.head()","metadata":{},"id":"50265658-3ac1-4075-bc11-46a313ddfa40","cell_type":"code","execution_count":null,"outputs":[]},{"source":"PROBABILITY: the predicted output of the model.  \nACTUAL_LABEL: the actual label of the test record.  ","metadata":{},"id":"3d5aff21-9f9a-40e3-911d-740cabf66411","cell_type":"markdown"},{"source":"First, we will visualize this dataset. This shows us that there will not be a perfect threshold; there is overlap between the datapoints and can thus not be seperated with one vertical line.","metadata":{},"id":"414c628f-e3b3-4621-a2eb-dc358d9da610","cell_type":"markdown"},{"source":"plt.scatter(df['PROBABILITY'], df['ACTUAL_LABEL'])\nplt.ylabel('ACTUAL_LABEL')\nplt.xlabel('PROBABILITY')\nplt.show()","metadata":{},"id":"f5c2d0ea-201f-4437-bd27-1eaee5fe67c6","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Now, we will plot the ROC curve using the scikit-learn's function `roc_curve`.","metadata":{},"id":"93a92fee-03da-4288-a665-1c769b86554d","cell_type":"markdown"},{"source":"preds = df['PROBABILITY']                          # Extract the output of our model from the data\ny_test = df['ACTUAL_LABEL']                        # Extract the correct label from the data\nfpr, tpr, threshold = roc_curve(y_test, preds)     # Using the `roc_curve' function from scikit-learnm calculate the FPRs, TPRs and thresolholds\nroc_auc = auc(fpr, tpr)                            # Calculate the AUROC (Area Under ROC Curve)\n\ndisplay = RocCurveDisplay(\n    fpr=fpr,\n    tpr=tpr,\n    roc_auc=roc_auc,\n)\n\ndisplay.plot()\nplt.show()","metadata":{},"id":"aeb163f5-fdbd-42fc-ac6a-08ff6821b626","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The red line indicates the TPR and FPR when we would perform random guessing. Luckily the model, which is indicated by the blue line, is above the red line and thus always better.  \nYou can now choose what threshold best suits your application.   \nExample:\nFor illness diagnosis, you want to be as cautious as possible. A very high TPR is thus necessary to always correctly identify sick people.\nFor spam detection, it is the other way around: we don't want important mails to end up in our spam.  \n\n\nFinally, we see that the AUROC of our model is 0.871. For reference: a perfect model has AUROC 1, while random guessing will have an AUROC of 0.5.","metadata":{},"id":"d54f6174-aa08-4ad1-ab8a-7ee231d5ac8d","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":5}