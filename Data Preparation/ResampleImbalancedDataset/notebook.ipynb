{"cells":[{"source":"# Resample an Imbalanced Dataset\nResampling is an important technique when you have a classification problem with an imbalance in classes. An imbalanced dataset can cause machine learning models to perform poorly and overpredict the majority class. In some settings, this type of bias can result in major costs (such as fraud detection and medical diagnosis). Balancing a dataset can be a way to avoid the issues that arise from a class imbalance and ensure that your classification model performs well.\n\nYou can use this template to resample a dataset of your own based on your needs.\n\nThe cell below imports the necessary libraries for this template, configures visualization settings, and defines a function to help interpret the outcomes of different resampling methods.","metadata":{},"id":"f17f169e-fcee-4b97-86f3-b1d23c55a2d9","cell_type":"markdown"},{"source":"# Data manipulation and visualization imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plotting size and style\nsns.set(rc = {'figure.figsize':(15, 8)})\nsns.set_style(\"darkgrid\")\n\n# Preprocessing imports\nfrom sklearn.model_selection import train_test_split\n\n# Create a function to plot resampled data\ndef plot_resample(X, y, X_res, y_res, method):    \n    # Create subplots\n    fig, axes = plt.subplot_mosaic([['a', 'b'], ['c', 'd']])\n\t# Generate scatterplots of first two features for inspection\n    sns.scatterplot(x=X.iloc[y.values == 0, 0], y=X.iloc[y.values == 0, 1], alpha=0.10, ax=axes['a'])\n    sns.scatterplot(x=X.iloc[y.values == 1, 0], y=X.iloc[y.values == 1, 1], alpha=0.10, ax=axes['a'])\n    sns.scatterplot(x=X_res.iloc[y_res.values == 0, 0], y=X_res.iloc[y_res.values == 0, 1], alpha=0.10, ax=axes['b'])\n    sns.scatterplot(x=X_res.iloc[y_res.values == 1, 0], y=X_res.iloc[y_res.values == 1, 1], alpha=0.10, ax=axes['b'])\n    sns.countplot(y=y, ax=axes['c'])\n    sns.countplot(y=y_res, ax=axes['d'])\n    # Set titles and show plot\n    plt.suptitle(method + ' Outcomes')\n    axes['a'].title.set_text('Original Data')\n    axes['b'].title.set_text(method + ' Data')\n    axes['c'].title.set_text('Original Class Proportions')\n    axes['d'].title.set_text(method + ' Class Proportions')\n    plt.tight_layout()\n    plt.show()","metadata":{},"id":"3a1afa67-51bb-42a3-968e-31995e7853de","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## Load in your data\nThe cell below is used to import the data. The example dataset used here is for fraud data where there is a large imbalance between fraudulent and non-fraudulent cases.\n\nðŸ‘‡&nbsp;&nbsp;_To use your own data, you will need to:_\n- _Upload a file and update the path provided to `pd.read_csv()`._\n- _Alternatively, if you have data in a database, you can add a SQL cell and connect to a custom integration._\n- _Your data will need to contain numeric data for the target and feature variables. It will also need to already be pre-processed (i.e., clean and with pre-processing steps already completed)._","metadata":{},"id":"c47093fd-cbed-477e-8a7c-74cd2ae06818","cell_type":"markdown"},{"source":"# Import the data (add your own file here)\ndf = pd.read_csv(\"data/fraud.csv\")\n\n# Preview the data\ndf","metadata":{},"id":"5b01ce5d-1b16-4ca9-9f96-2fa0b73c3b46","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## How imbalanced is my data?\nThe first step is to prepare the data and investigate the extent of the imbalance. This includes splitting your data into training and testing subsets and then visualizing the imbalance in your target variable.\n\nðŸ‘‡&nbsp;&nbsp;_In this cell, you will need to:_\n- _Set the name of the target variable._\n- _Define which variables are your features._","metadata":{},"id":"0be262dd-c363-4f7d-837b-e19eb3bd8b88","cell_type":"markdown"},{"source":"# Set target and feature variables (add your own here)\ntarget = \"is_fraud\"\nfeatures = [\"lat\", \"long\", \"amt\", \"city_pop\", \"merch_lat\", \"merch_long\"]\n\n# Separate the target from the features\ny = df[target]\nX = df[features]\n\n# Split the data into train and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Plot the counts of the \nsns.countplot(y=y_train) \nplt.title(\"Imbalance Between Classes\", size=16)\nplt.show()","metadata":{},"id":"bb33383d-df34-433b-ae9f-fad1b50757c9","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Based on the size and type of your data, you will next want to choose a method to correct the imbalance between classes. You can navigate to the following sections to find the code to:\n- Perform [random under-sampling](#rus) to reduce the quantity of the majority class.\n- Perform [random over-sampling](#ros) to increase the quantity of the minority class.\n- Perform [SMOTE](#smote) over-sampling to increase the quantity of the minority class.","metadata":{},"id":"2253fe78-5cfc-419a-8fa3-5f4e24954d41","cell_type":"markdown"},{"source":"<a id='rus'></a>\n## Randomly sampling our majority class to match the size of the minority\nRandom under-sampling is a technique that performs random draws from the majority class to match the minority class. As a result, you risk throwing away a lot of data. You will probably only want to use random under-sampling when:\n- You have a lot of data that you can afford to discard.\n- You are aiming for computational efficiency.\nThe following code uses [RandomUnderSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) to under-sample the majority class to match the minority class.\n\nðŸ’¡&nbsp;&nbsp;_For `RandomUnderSampler()`, and the other sampling classes introduced in this template, there is an optional `sampling_strategy` parameter. Passing a float between 0 and 1 as an argument will adjust the ratio of samples in the minority class._","metadata":{},"id":"b4818df6-e968-4fae-a612-5c04263fa40d","cell_type":"markdown"},{"source":"# Import RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Resample the data\nrus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X_train, y_train)\n\n# View the resampled labels\nplot_resample(X_train, y_train, X_res, y_res, \"Random Under Sampling\")","metadata":{},"id":"e308988b-cacd-48c8-af60-0d687b7bbb2d","cell_type":"code","execution_count":null,"outputs":[]},{"source":"ðŸ’¡&nbsp;&nbsp;_Points in the upper plots are slightly transparent, which means that highly opaque points indicate a greater concentration of values. Notice how there is greater transparency in the random undersampled data, indicating fewer data points (confirmed by the lower right barplot) and a greater balance of the green and markers, indicating an equal quantity between classes._","metadata":{},"id":"3a8eec87-496f-49f7-a4b9-76b5717286ed","cell_type":"markdown"},{"source":"<a id='ros'></a>\n## Randomly sampling to increase the size of our minority class\nIf you wish to avoid the loss of data that comes with random under-sampling, you can choose to use random over-sampling. This technique performs random draws with replacement on the minority class to match the majority class. This is a simple technique but has the drawback that your model will be trained on a large number of duplicates which may harm model performance by introducing issues such as overfitting.\n\nThe following code uses [RandomOverSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) to over-sample the minority class to match the majority class.","metadata":{},"id":"bf0565fe-5a62-47a8-bea6-6bf371ac5209","cell_type":"markdown"},{"source":"# Import RandomOverSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Resample the data\nros = RandomOverSampler(random_state=42)\nX_res, y_res = ros.fit_resample(X_train, y_train)\n\n# View the resampled labels\nplot_resample(X_train, y_train, X_res, y_res, \"Random Over Sampling\")","metadata":{},"id":"5c43e680-7f3d-43d1-abf3-5086dabf8b53","cell_type":"code","execution_count":null,"outputs":[]},{"source":"ðŸ’¡&nbsp;&nbsp;_Notice here that unlike the random under sampling, there is a similar level of opacity between the two plots. This indicates that the quantity of data was preserved after resampling. There is also now a similar quantity of orange and blue points, indicating an equality between the two classes (confirmed by the lower right bar plot)._","metadata":{},"id":"1f1a351f-9501-4eae-88cb-5471f1144078","cell_type":"markdown"},{"source":"<a id='smote'></a>\n## Creating synthetic samples to rebalance our data\nThe final technique this template will cover is SMOTE or Synthetic Minority Over-sampling Technique. SMOTE functions works like random over-sampling, but instead of exact duplicates, it creates new synthetic observations. Although this reduces the risk of overfitting, it can have some potential downsides:\n- SMOTE does not work as well with high-dimensional data.\n- SMOTE can introduce noise into the data because of how it generates synthetic examples.\n\nThe following code uses [SMOTE()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) to use examples from the minority class to generate new synthetic data points to match the majority class.\n\nðŸ’¡&nbsp;&nbsp;_If your dataset contains categorical features, such as one-hot encoded variables, you will need to use [SMOTENC](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html) instead of SMOTE. Visit the documentation to learn more._","metadata":{},"id":"3781b886-e735-4ab9-93fa-2ef00845249e","cell_type":"markdown"},{"source":"# Import SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Resample the data\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# View the resampled labels\nplot_resample(X_train, y_train, X_res, y_res, \"SMOTE\")","metadata":{},"id":"ffb572d4-bb27-484a-a96f-cc2e7e9f4f38","cell_type":"code","execution_count":null,"outputs":[]},{"source":"ðŸ’¡&nbsp;&nbsp;_You may notice two features of the plot above:_\n- _Lines of points in the plot of the SMOTE data. This is caused by the way in which the new samples are generated through interpolation. You can read more about how SMOTE works [here](https://imbalanced-learn.org/stable/over_sampling.html#smote-adasyn)._\n- _An abundance of the minority class. This is because in the example data there are many duplicate values. Via interpolation, the minority class will have fewer duplicates, and will therefore cover more of the plot._","metadata":{},"id":"89cafef4-f1d7-48bc-b51a-cc22ec140fa5","cell_type":"markdown"},{"source":"## Further reading\nThere are a number of ways to proceed, including:\n- Applied use cases where you might encounter class imbalances, such as fraud detection. DataCamp's [Fraud Detection in Python](https://app.datacamp.com/learn/courses/fraud-detection-in-python) covers this topic in greater depth.\n- More advanced forms of sampling methods, including combinations such as [SMOTEEN](https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html#imblearn.combine.SMOTEENN) and [SMOTETomek](https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTETomek.html#imblearn.combine.SMOTETomek).\n- With a balanced dataset, you may want to check out [workspace templates](https://app.datacamp.com/workspace/templates?selectedLabels=%5B%22classification%22%5D) focused on classification problems.","metadata":{},"id":"d650da65-e151-4740-be68-c68a89408aee","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}