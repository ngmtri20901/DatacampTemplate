{"cells":[{"source":"# Predict CTR and Evaluate ROI","metadata":{},"id":"e9c78c83-1aec-4d06-b80b-2ab703343c27","cell_type":"markdown"},{"source":"Click-through-rate, or CTR, is a metric that defines how many clicks an ad receives divided by the number of views or impressions. Accurately predicting CTR is critical to media-buying decisions because it helps ensure ads are targeted to the right users. This template predicts click-through rates based on categorical and numeric features. It also calculates the cost, return, and return on investment (ROI) based on predicted clicks. This template is designed for those interested in how to optimize ads with machine learning.\n\nTo use this template, you will need a dataset containing click log data that meets the following conditions:\n- There are at least two feature columns and a column with a binary target variable indicating whether the user clicked or not (0 or 1).\n- Any additional features you want to include have already been created. You can review this [course](https://app.datacamp.com/learn/courses/feature-engineering-for-machine-learning-in-python) for more information on feature engineering.\n- There are no NaN/NA values. You can use [this template to impute missing values](https://app.datacamp.com/workspace/templates/recipe-python-impute-missing-data) if needed.\n\nThe placeholder dataset in this template consists of web browser data, including the search engine type, position of the ad, and whether or not the user clicked.","metadata":{},"id":"27be9347-9e6f-4811-a1eb-b09638bbb272","cell_type":"markdown"},{"source":"## 1. Loading packages and data","metadata":{},"id":"edf21890","cell_type":"markdown"},{"source":"# Load packages\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n# Replace with your CSV file path and load the data\ndf = pd.read_csv(\"data/ctr_data.csv\")\n\n# Preview the data\ndf","metadata":{"scrolled":true},"id":"336d7477-fda2-4903-9ec9-ef2f75349094","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The `pandas` method [`.info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) prints a summary of the data. This includes the data types and the number of non-missing values. The summary is helpful to understand what types of pre-processing may be necessary.","metadata":{},"id":"af5df0c1","cell_type":"markdown"},{"source":"df.info()","metadata":{},"id":"4fad473c","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 2. Pre-processing the Data","metadata":{},"id":"7a3949a6","cell_type":"markdown"},{"source":"The code below uses sklearn's [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to process the numeric and categorical data in preparation for a machine learning model.\n\n- Standardizing numeric columns is important for some machine learning models. The code below removes skew using [`PowerTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html), and then scales it using [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n- The categorical data is one-hot encoded using [`OneHotEncoder()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot#sklearn.preprocessing.OneHotEncoder), which creates new binary columns for each category in a column. \n\n_Note: Three numeric columns, \"search_engine_type_count\", \"product_type_count\", and \"advertiser_type_count\", were created for the purposes of this template. Although feature engineering is outside the scope of this template, you can review how they were created [here](https://campus.datacamp.com/courses/predicting-ctr-with-machine-learning-in-python/exploratory-ctr-data-analysis?ex=5)._","metadata":{},"id":"28056721","cell_type":"markdown"},{"source":"# Specify the numeric columns you wish to process\nnumeric_features = [\n    \"search_engine_type_count\",\n    \"product_type_count\",\n    \"advertiser_type_count\",\n]\n\n# Specify the categorical columns you wish to process\ncategorical_features = [\n    \"banner_pos\",\n    \"device_type\",\n    \"device_conn_type\",\n    \"product_type\",\n    \"advertiser_type\",\n]\n\n# Transform the numeric and categorical columns\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"boxcox\", PowerTransformer(method=\"box-cox\", standardize=False)),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n    ]\n)\n\nprocessed_features = pd.DataFrame(\n    preprocessor.fit_transform(df), columns=preprocessor.get_feature_names_out()\n)\n\n# Preview the processed DataFrame\nprocessed_features","metadata":{},"id":"ede47b15","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Finally, the data is split into the target and feature variables, and then further split into training and testing datasets using [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).","metadata":{},"id":"22b23656","cell_type":"markdown"},{"source":"# Select the processed columns\nX = processed_features\n\n# Select the column you wish to use as a target\ny = df[\"click\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{},"id":"78c1c6db","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 3. Setting a Baseline and Evaluation Metrics\nThe following evaluation assumes a bidding process where you bid on impressions that you predict will result in a click and do not bid on impressions that you predict will not result in a click. The code below defines two functions to evaluate your model. The first function calculates the CTR metrics and takes as arguments:\n- The cost that you expect to pay per X number of impressions (`c`). Typically, impressions are charged as a cost per 1000 impressions.\n- The downstream return per click `r` (e.g., based on the chance a customer purchases a product).\n- The true values (`true_values`).\n- The predicted values (`predictions`) based on your model.\nThe function then calculates the total return, total cost, and total return on investment (ROI) of the model. \n\nThe second function accepts two of the same arguments (`true_values` and `predictions`), and generates two classification metrics to help you evaluate the model:\n- Precision: the proportion of clicks you correctly predicted relative to the total number of impressions you predicted to result in a click (i.e., including false positives). Maximizing this increases your ROI on ad spend.\n- Recall: the proportion of clicks you correctly predicted relative to the total number of clicks that occurred (including ones you missed). Maximizing this means that the ads are targeting the right people.\n\nThe code also sets a baseline to evaluate a model with no bids (i.e., every event is a non-click). If you want to learn more about evaluating your CTR prediction model, be sure to check out these two videos on [CTR metrics](https://campus.datacamp.com/courses/predicting-ctr-with-machine-learning-in-python/model-applications-and-improvements?ex=1) and [model evaluation](https://campus.datacamp.com/courses/predicting-ctr-with-machine-learning-in-python/model-applications-and-improvements?ex=5).","metadata":{},"id":"79edefd3","cell_type":"markdown"},{"source":"# Set the costs and returns you expect for clicks and impressions here!\nc = 0.05\nr = 0.2\n\n# Define a metric to calculate CTR metrics\ndef calculate_ctr_metrics(true_values, predictions, r, c):\n\n    # Generate a confusion matrix\n    conf_matrix = confusion_matrix(true_values, predictions)\n    tn, fp, fn, tp = conf_matrix.ravel()\n\n    # Calculate the total return, cost, and roi\n    total_return = tp * r\n    total_cost = (tp + fp) * c\n    if total_cost != 0:\n        roi = total_return / total_cost\n    else:\n        roi = 0.0\n\n    # Return the metrics as a dictionary\n    ctr_metrics = {\n        \"total_return\": round(total_return, 2),\n        \"total_cost\": round(total_cost, 2),\n        \"roi\": round(roi, 2),\n    }\n    return ctr_metrics\n\n\ndef calculate_model_metrics(true_values, predictions):\n    \n    # Calculate the precision and the recall\n    prec = precision_score(\n        true_values, predictions, average=\"weighted\", zero_division=0\n    )\n    recall = recall_score(true_values, predictions, average=\"weighted\")\n\n    # Return the metrics as a dictionary\n    model_metrics = {\"precision\": round(prec, 5), 'recall': round(recall, 5)}\n    return model_metrics\n\n\n# Set up baseline data containing no bids\nbaseline_pred = np.asarray([0 for x in range(len(X_test))])\n\n# Evaluate the performance with no bids\nbaseline_metrics = calculate_model_metrics(true_values=y_test, predictions=baseline_pred)\n\nprint(\"Precision:\", baseline_metrics['precision'])\nprint(\"Recall:\", baseline_metrics['recall'])","metadata":{"scrolled":true},"id":"bc29b53b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 4. Fitting and Tuning  a Model\nWith a baseline established, it is now possible to build a classification model and evaluate its performance. The following code uses a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [`RandomizedSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to initialize a random forest classifier and perform a randomized search on hyperparameters.\n\nThe classifier returns predictions based on a `threshold` that you can customize. This threshold will determine at what probability an event is classified as a click or not. For example, you may want to increase the threshold when costs are high so that you only spend when you are confident in a click. \n\nBy default the threshold is set at 0.25. Try adjusting the threshold to see how it affects the CTR and model metrics!\n\n_Note: The randomized grid search may take some time to run._","metadata":{},"id":"168439fe","cell_type":"markdown"},{"source":"# Set the threshold\nthreshold = 0.25\n\n# Set up the parameters to sample from\nparam_grid = {\"max_depth\": list(range(5, 20)), \"max_features\": [\"auto\", \"sqrt\"]}\n\n# Instantiate a RandomForestClassifier and Randomized SearchCV\nrf = RandomForestClassifier()\n\nrandom_rf_class = RandomizedSearchCV(\n    estimator=rf,\n    param_distributions=param_grid,\n    n_iter=3,\n    scoring=\"roc_auc\",\n    n_jobs=4,\n    cv=3,\n)\n\n# Fit it to the data\nrandom_rf_class.fit(X_train, y_train)\n\n# Create predictions\nrandom_rf_pred = (\n    random_rf_class.best_estimator_.predict_proba(X_test)[:, 1] >= threshold\n).astype(bool)\n\n# Evaluate the performance of the model\nctr_metrics = calculate_ctr_metrics(\n    true_values=y_test,\n    predictions=random_rf_pred,\n    r=r,\n    c=c,\n)\n\nmodel_metrics = calculate_model_metrics(true_values=y_test, predictions=random_rf_pred)\n\nprint(\"Total Return:\", ctr_metrics[\"total_return\"])\nprint(\"Total Cost:\", ctr_metrics[\"total_cost\"])\nprint(\"ROI:\", ctr_metrics[\"roi\"])\nprint(\"Precision:\", model_metrics[\"precision\"])\nprint(\"Recall:\", model_metrics[\"recall\"])","metadata":{},"id":"06955cae","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 5. Plot the ROI for Different Costs and Returns\nFinally, it is possible to visualize the effect that different returns (`r`) and costs (`c`) have on the ROI. The following code generates an interactive plot that shows the ROI for different levels of returns. You can further visualize the additional effect of costs levels by using the slider at the bottom. \n\nAs you might expect, the lowest costs and the highest returns generate the greatest ROI. Feel free to experiment with the different returns and costs by editing the lists below!","metadata":{},"id":"imperial-threat","cell_type":"markdown"},{"source":"# Create figure\nfig = go.Figure()\n\n# Set costs and returns\nreturns = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\ncosts = [0.05, 0.1, 0.15, 0.2, 0.25, 0.30]\n\n# Add traces, one for each cost\nfor cost in costs:\n    roi = [\n        calculate_ctr_metrics(\n            true_values=y_test, predictions=random_rf_pred, r=ret, c=cost\n        )[\"roi\"]\n        for ret in returns\n    ]\n    fig.add_trace(\n        go.Scatter(visible=False, x=returns, y=roi, hoverinfo=\"text\", hovertext=roi)\n    )\n\n# Make first trace visible\nfig.data[1].visible = True\n\n# Create and add slider\nsteps = []\nfor i in range(len(costs)):\n    step = dict(\n        method=\"update\",\n        args=[{\"visible\": [False] * len(fig.data)}],\n        label=costs[i],  # layout attribute\n    )\n    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n    steps.append(step)\n\nsliders = [\n    dict(active=1, currentvalue={\"prefix\": \"Cost: \"}, pad={\"t\": 50}, steps=steps)\n]\n\n# Update the layout and show the figure\nfig.update_layout(\n    sliders=sliders,\n    title=\"ROI for Different Levels of Returns and Costs\",\n    xaxis_title=\"Returns\",\n    yaxis_title=\"ROI\",\n)\nfig.update_yaxes(autorange=False)\n\nfig.show()","metadata":{},"id":"amber-december","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 6. Next Steps\nYou can further adjust this template by experimenting with different models and adjusting the cost and return parameters to calculate predicted ROI! If you want to learn more about using machine learning to predict CTR in Python, be sure to check out [this course](https://app.datacamp.com/learn/courses/predicting-ctr-with-machine-learning-in-python).\n\nNote that the model used above serves as an example and may not be the best model for your data. You may want to check out the following courses to learn more about classification and tuning models:\n- [Supervised Learning with scikit-learn](https://app.datacamp.com/learn/courses/supervised-learning-with-scikit-learn)\n- [Machine Learning with Tree-Based Models in Python](https://app.datacamp.com/learn/courses/machine-learning-with-tree-based-models-in-python)\n- [Hyperparameter Tuning in Python](https://app.datacamp.com/learn/courses/hyperparameter-tuning-in-python)","metadata":{},"id":"1fe34fb0","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6rc1"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":5}