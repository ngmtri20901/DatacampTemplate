{"cells":[{"source":"# Customer Segmentation Report\nCustomer segmentation is a way to identify groups of similar customers. Customers can be segmented on a wide variety of characteristics, such as demographic information, purchase behavior, and attitudes. This template provides an end-to-end report for processing and segmenting customer purchase data using a K-means clustering algorithm. It also includes a snake plot and heatmap to visualize the resulting clusters and feature importance.\n\nTo use your data, the following criteria must be satisfied:\n- Multiple numerical variables that you can use for clustering.\n- No NaN/NA values. You can use [this template to impute missing values](https://app.datacamp.com/workspace/templates/recipe-python-impute-missing-data) if needed.\n\nThe placeholder dataset in this template consists of customer data, including purchase recency, frequency, and monetary value. Each row represents a different customer with a distinct customer ID.","metadata":{},"id":"e9c78c83-1aec-4d06-b80b-2ab703343c27","cell_type":"markdown"},{"source":"## 1. Loading packages and Inspecting the Data\nThe code below imports the packages necessary for data manipulation, visualization, pre-processing, and clustering. It also sets up the visualization style and loads in the data.\n\nFinally, it inspects the data types and missing values with the [`.info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method from `pandas`.","metadata":{},"id":"edf21890","cell_type":"markdown"},{"source":"# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Set visualization style\nsns.set_style(\"darkgrid\")\n\n# Load the data and replace with your CSV file path\ndf = pd.read_csv(\"data/customer_data.csv\")\n\n# Preview the data\ndf","metadata":{},"id":"336d7477-fda2-4903-9ec9-ef2f75349094","cell_type":"code","execution_count":null,"outputs":[]},{"source":"# Check columns for data types and missing values\ndf.info()","metadata":{"scrolled":true},"id":"e2101794-8fa6-48d7-bc30-33a2cbfb303d","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 2. Exploring the Data\nBased on the evaluation above, you can select columns that you wish to inspect further. In this template, three columns are selected from the four columns. CustomerID is omitted because it is an identifier and not useful for clustering.\n\nThe code below reduces the DataFrame to the columns you wish to cluster on and then prints descriptive statistics using the [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method from `pandas`.\n\nPrinting descriptive statistics is helpful because K-means clustering has several key assumptions that can be revealed via this exploration:\n1. There is no skewness to the data.\n2. The variables have the same average values.\n3. The variables have the same variance.\n\nIf you'd like to learn more about pre-processing data for K-means clustering, you can refer to this [video](https://campus.datacamp.com/courses/customer-segmentation-in-python/data-pre-processing-for-clustering?ex=1) from the course Customer Segmentation in Python.","metadata":{},"id":"28e70c71","cell_type":"markdown"},{"source":"# Select columns for clustering\ncolumns_for_clustering = [\"Recency\", \"Frequency\", \"MonetaryValue\"]\n\n# Create new DataFrame with clustering variables\ndf_features = df[columns_for_clustering]\n\n# Print a summary of descriptive statistics\ndf_features.describe()","metadata":{},"id":"prescription-identity","cell_type":"code","execution_count":null,"outputs":[]},{"source":"The [`facetgrid()`](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) function from `seaborn` creates a grid of histograms of the data to be clustered.  It serves as a further exploration of the data to determine its skew and whether it needs transformation.","metadata":{},"id":"indie-psychiatry","cell_type":"markdown"},{"source":"# Plot the distributions of the selected variables\ng = sns.FacetGrid(\n    df_features.melt(),  # Reformat the DataFrame for plotting purposes\n    col=\"variable\",  # Split on the 'variable' column created by reformating\n    sharey=False,  # Turn off shared y-axis\n    sharex=False,  # Turn off shared x-axis\n)\n# Apply a histogram to the facet grid\ng.map(sns.histplot, \"value\")\n# Adjust the top of the plots to make room for the title\ng.fig.subplots_adjust(top=0.8)\n# Create a title\ng.fig.suptitle(\"Unprocessed Variable Distributions\", fontsize=16)\nplt.show()","metadata":{},"id":"13b6d506","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Before proceeding, it is crucial to ensure that all columns selected for clustering are numeric. The following code iterates through the reduced DataFrame and checks whether each column is numeric. If it returns `True`, then you can proceed with the pre-processing.","metadata":{},"id":"0a5ea05a","cell_type":"markdown"},{"source":"all([pd.api.types.is_numeric_dtype(df_features[col]) for col in columns_for_clustering])","metadata":{},"id":"81a16dd2","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 3. Pre-processing the Data\nBased on the grids above, if there is a skew, you will have to complete this step which removes the skew and center the variables. This is the case for the placeholder dataset used in this template and will likely be the case for your data.\n- First, a log transformation is applied to the data using the `numpy` [`log()`](https://numpy.org/doc/stable/reference/generated/numpy.log.html) function. A log transformation unskews the data in preparation for clustering.\n- Next, the `StandardScaler()` from [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) fits and transforms the log-transformed data. This centers and scales the data in further preparation for clustering.\n- Finally, a new DataFrame is created and visualized again to confirm the results.","metadata":{},"id":"f2bad38a","cell_type":"markdown"},{"source":"# Perform a log transformation of the data to unskew the data\ndf_log = np.log(df_features)\n\n# Initialize a standard scaler and fit it\nscaler = StandardScaler()\nscaler.fit(df_log)\n\n# Scale and center the data\ndf_normalized = scaler.transform(df_log)\n\n# Create a pandas DataFrame of the processed data\ndf_processed = pd.DataFrame(\n    data=df_normalized, index=df_features.index, columns=df_features.columns\n)\n\n# Plot the distributions of the selected variables\ng = sns.FacetGrid(df_processed.melt(), col=\"variable\")\ng.map(sns.histplot, \"value\")\ng.fig.subplots_adjust(top=0.8)\ng.fig.suptitle(\"Preprocessed Variable Distributions\", fontsize=16)\nplt.show()","metadata":{},"id":"74e3d50b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 4. Choosing the Number of Clusters","metadata":{},"id":"cc5abde3-eda6-40b5-bde7-25a6477373da","cell_type":"markdown"},{"source":"The next step is to fit a variable number of clusters and plot each cluster's sum-of-squared errors (SSE). The SSE reflects the sum of squared distances from every data point to the cluster center. The aim is to reduce the SSE while still maintaining a reasonable number of clusters. \n\nBy plotting the SSE for each number of clusters, you can identify at what point there are diminishing returns by adding new clusters. This type of plot is called an elbow plot.\n\nIn the code below, you can set the maximum number of clusters you want to plot, and then a loop is used to generate the SSE for each number of clusters. Finally, the `seaborn` function [`pointplot()`](https://seaborn.pydata.org/generated/seaborn.pointplot.html) plots a curve with each cluster number and SSE. This allows you to identify the 'elbow' or point where there are only marginal reductions for each additional cluster.","metadata":{},"id":"2fca1908-736b-4f0b-9966-a90118863279","cell_type":"markdown"},{"source":"# Set the maximum number of clusters to plot\nmax_clusters = 10\n\n# Initialize empty dictionary to store sum of squared errors\nsse = {}\n\n# Fit KMeans and calculate SSE for each k\nfor k in range(1, max_clusters):\n    # Initialize KMeans with k clusters\n    kmeans = KMeans(n_clusters=k, random_state=1)\n    # Fit KMeans on the normalized dataset\n    kmeans.fit(df_processed)\n    # Assign sum of squared distances to k element of dictionary\n    sse[k] = kmeans.inertia_\n\n# Initialize a figure of set size\nplt.figure(figsize=(10, 4))\n\n# Create an elbow plot of SSE values for each key in the dictionary\nsns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n\n# Add labels to the plot\nplt.title(\"Elbow Method Plot\", fontsize=16)  # Add a title to the plot\nplt.xlabel(\"Number of Clusters\")  # Add x-axis label\nplt.ylabel(\"SSE\")  # Add y-axis label\n\n# Show the plot\nplt.show()","metadata":{"scrolled":true},"id":"32af5116-21c3-43cb-aa82-9febfd29dec9","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 5. Clustering the Data\nYou can now select an optimal number of clusters based on the elbow plot above by setting `k`. In this example, `k` is set to 3.\n\n[`KMeans()`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from `sklearn.cluster` with `k` clusters is then fit to the processed data, and the cluster labels are extracted and assigned back to the original data. This allows you to inspect raw data by cluster in later steps.","metadata":{},"id":"5c9c8d6a","cell_type":"markdown"},{"source":"# Choose number of clusters\nk = 3\n\n# Initialize KMeans\nkmeans = KMeans(n_clusters=k, random_state=1) \n\n# Fit k-means clustering on the normalized data set\nkmeans.fit(df_processed)\n\n# Extract cluster labels\ncluster_labels = kmeans.labels_\n\n# Create a new DataFrame by adding a new cluster column to the original data\ndf_clustered = df.assign(Cluster=cluster_labels)\n\n# Preview the clustered DataFrame\ndf_clustered","metadata":{},"id":"e8cd11d7","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 6. Inspecting the Clusters\n### 6a. Visualizing the Raw Values by Cluster\nThe next step is to analyze the unprocessed data by cluster. The `pandas` method [`DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html), combined with the [`.size()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.size.html) method, returns the total number of rows per `Cluster`.","metadata":{},"id":"21b0f9a2","cell_type":"markdown"},{"source":"# Group the data by cluster and calculate the total number of rows per group\ndf_sizes = df_clustered.groupby([\"Cluster\"], as_index=False).size()\n\n# Inspect the row counts\ndf_sizes","metadata":{},"id":"e5030df1","cell_type":"code","execution_count":null,"outputs":[]},{"source":"Next, the mean values per cluster are visualized. The data is grouped again, and this time, the `pandas` method [`.mean()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.mean.html) is used to aggregate the data by cluster and calculate the mean for each variable. Alternatively, the [`.agg()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) method can also be used to specify specific aggregations for different columns if necessary. Consult the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) for further information on the types of aggregations possible.\n\nThe `seaborn` [`catplot()`](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot) function visualizes the means per cluster.","metadata":{},"id":"c1cdd263","cell_type":"markdown"},{"source":"# Calculate the mean of feature columns by cluster\ndf_means = df_clustered.groupby([\"Cluster\"])[df_features.columns].mean().reset_index()\n\n# Plot the distributions of the selected variables\nsns.catplot(\n    data=df_means.melt(id_vars=\"Cluster\"),  # Transform the data to enable plotting\n    col=\"variable\",\n    x=\"Cluster\",\n    y=\"value\",\n    kind=\"bar\",\n)\n\n# Add a title\nplt.suptitle(\"Average Values by Cluster\", y=1.04, fontsize=16)\n\n# Show the plot\nplt.show()","metadata":{"scrolled":true},"id":"be2675e3","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### 6b. Create a Snake Plot of the Clusters\nThe next step takes the processed data and visualizes the differences between the clusters using a snake plot. This can be helpful spot trends or key differences that would not be visible with the raw data. The code below uses the following code:\n- [`DataFrame.assign()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html) adds the cluster labels to the processed data.\n- [`DataFrame.melt()`](https://pandas.pydata.org/pandas-docs/version/1.0.0/reference/api/pandas.DataFrame.melt.html) transforms the data from wide to long format, which makes plotting easier.\n- Finally, the `seaborn` [`lineplot()`](https://seaborn.pydata.org/generated/seaborn.lineplot.html) function is used to plot three lines, one for each cluster.","metadata":{},"id":"274ea070","cell_type":"markdown"},{"source":"# Assign cluster labels to processed DataFrame\ndf_processed_clustered = df_processed.assign(Cluster=cluster_labels)\n\n# Melt the normalized DataFrame and reset the index\ndf_processed_melt = pd.melt(\n    df_processed_clustered.reset_index(),\n    # Assign the cluster labelss as the ID\n    id_vars=['Cluster'],\n    # Assign clustering variables as values\n    value_vars=df_features.columns,\n    # Name the variable and value\n    var_name=\"Metric\",\n    value_name=\"Value\",\n)\n\n# Change the figure size\nplt.figure(figsize=(10, 6))\n\n# Add label and titles to the plot\nplt.title('Snake Plot of Normalized Variables', fontsize=16)\nplt.xlabel('Metric')\nplt.ylabel('Average Normalized Value')\n\n# Plot a line for each value of the cluster variable\nsns.lineplot(data=df_processed_melt, x='Metric', y='Value', hue='Cluster')\nplt.show()","metadata":{},"id":"a98cba3c","cell_type":"code","execution_count":null,"outputs":[]},{"source":"### 6c. Create a Heatmap of Relative Importances\nAnother technique to help visualize how each segment is distinct is to plot the relative importance. The code below achieves this by doing the following:\n- First, it calculates the average values for each cluster.\n- Next, it calculates the average values for the total population.\n- It then divides the cluster averages by the population averages and subtracts one.\n\nThis provides a relative importance score for each of the different features used for clustering. The `seaborn` [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function plots these relative importances on a red to blue color scale to help visualize the relative importance of each attribute to the segments.","metadata":{},"id":"9fe87e0a","cell_type":"markdown"},{"source":"# Calculate average RFM values for each cluster\ncluster_avg = df_clustered.groupby([\"Cluster\"])[columns_for_clustering].mean()\n\n# Calculate average RFM values for the total customer population\npopulation_avg = df[columns_for_clustering].mean()\n\n# Calculate relative importance of cluster's attribute value compared to population\nrelative_imp = cluster_avg / population_avg - 1\n\n# Change the figure size\nplt.figure(figsize=(8, 4))\n\n# Add the plot title\nplt.title(\"Relative importance of attributes\", fontsize=16)\n\n# Plot the heatmap\nsns.heatmap(data=relative_imp, annot=True, fmt=\".2f\", cmap=sns.diverging_palette(20, 220, as_cmap=True))\nplt.show()","metadata":{"scrolled":true},"id":"9b72196b","cell_type":"code","execution_count":null,"outputs":[]},{"source":"This concludes the report! If you are still interested in this topic, there are a number of great DataCamp resources for you to explore:\n- If you would like to learn more about customer segmentation, take a look at DataCamp's [Customer Segmentation in Python](https://app.datacamp.com/learn/courses/customer-segmentation-in-python) course.\n- If you are interested in other clustering methods, be sure to look into the course [Cluster Analysis in Python](https://app.datacamp.com/learn/courses/cluster-analysis-in-python).\n- Finally, for more information on unsupervised learning in general, you can take [Unsupervised Learning in Python](https://app.datacamp.com/learn/courses/unsupervised-learning-in-python).\n\nHappy clustering!","metadata":{},"id":"6fbed31c","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":5}